from abc import ABC, abstractmethod

class BaseBenchmark(ABC):
    """
    Abstract base class for LLM benchmarks.
    """
    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    def get_questions(self):
        """
        Loads or defines the benchmark questions.
        Each question should ideally be a dictionary or object that
        can contain the prompt, expected answer/format, and evaluation criteria.
        
        Returns:
            list: A list of questions.
        """
        pass

    @abstractmethod
    def evaluate(self, model_response: str, question_data: dict) -> (float | None):
        """
        Evaluates the model's response against the expected criteria for a single question.

        Args:
            model_response (str): The text generated by the LLM.
            question_data (dict): The data associated with the question,
                                  including what's needed for evaluation.

        Returns:
            float or None: The score for this particular question (e.g., 0.0 or 1.0 for binary,
                           or a scaled score). Returns None if evaluation is not possible.
        """
        pass

    def get_name(self) -> str:
        """Returns the name of the benchmark."""
        return self.name